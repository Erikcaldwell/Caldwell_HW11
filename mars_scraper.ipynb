{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up to scrape the NASA website. Assigning the variables for reference later.\n",
    "\n",
    "# empty dictionary and empty list for future use\n",
    "news_data = {}\n",
    "paragraph_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url for the target site\n",
    "base_url = \"https://mars.nasa.gov/\" #anything special for https??\n",
    "\n",
    "#starting scrape here\n",
    "nasa_url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "#first response URL here...think so?\n",
    "response_1 = req.get(nasa_url)\n",
    "\n",
    "# ahh beautiful soup; running everything through its easy button\n",
    "nasa_soup = bs(response_1.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not making this easy - searching for what we need\n",
    "\n",
    "#looking for classes\n",
    "soup_div = nasa_soup.find(class_=\"slide\")\n",
    "\n",
    "#looking for anchors\n",
    "soup_news = soup_div.find_all('a')\n",
    "\n",
    "#pulling and cleaning\n",
    "news_title = soup_news[1].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding paragraphs, connecting, gets next response and then sends to bs\n",
    "soup_p = soup_div.find_all('a', href=True)\n",
    "soup_p_url = soup_p[0]['href']\n",
    "paragraph_url = base_url + soup_p_url\n",
    "response_2 = req.get(paragraph_url)                                          \n",
    "para_soup = bs(response_2.text, \"html.parser\")                               \n",
    "ww_paragraphs = para_soup.find(class_='wysiwyg_content')                     \n",
    "paragraphs = ww_paragraphs.find_all('p') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning paragraphs and adding to the list\n",
    "for paragraph in paragraphs:                                                 \n",
    "    clean_paragraph = paragraph.get_text().strip()                               \n",
    "    paragraph_text.append(clean_paragraph)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_title': 'Opportunity Hunkers Down During Dust Storm',\n",
       " 'paragraph_text_1': 'NASA Mars Exploration Rover Status Report',\n",
       " 'paragraph_text_2': 'Updated at 2:25 p.m. PDT on July 26, 2018'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding items to the dictionary\n",
    "news_data[\"news_title\"] = news_title                                        \n",
    "news_data[\"paragraph_text_1\"] = paragraph_text[0]\n",
    "news_data[\"paragraph_text_2\"] = paragraph_text[1] \n",
    "news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "browser = Browser('chrome', headless=False)                                  \n",
    "jpl_fullsize_url = 'https://photojournal.jpl.nasa.gov/jpeg/'                 \n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visits search URL with automated browser\n",
    "# acquires response from URL\n",
    "# sends response to beautiful soup\n",
    "browser.visit(jpl_url)                                                       \n",
    "jpl_html = browser.html                                                      \n",
    "jpl_soup = bs(jpl_html, 'html.parser')                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all the images and putting them / appending them to the list\n",
    "featured_image_list = []                                                     \n",
    "\n",
    "for image in jpl_soup.find_all('div',class_=\"img\"):                          \n",
    "    featured_image_list.append(image.find('img').get('src')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts first image found and cleaning them up\n",
    "feature_image = featured_image_list[0]                                       \n",
    "temp_list_1 = feature_image.split('-')                                       \n",
    "temp_list_2 = temp_list_1[0].split('/')                                      \n",
    "featured_image_url = jpl_fullsize_url + temp_list_2[-1] + '.jpg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_url\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the weather data and twitter data\n",
    "browser = Browser('chrome', headless=False)                                  \n",
    "tweet_url = 'https://twitter.com/marswxreport?lang=en'                       \n",
    "browser.visit(tweet_url)  \n",
    "tweet_html = browser.html                                                   \n",
    "tweet_soup = bs(tweet_html, 'html.parser') \n",
    "weather_info_list = []    \n",
    "for weather_info in tweet_soup.find_all('p',class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"):\n",
    "    weather_info_list.append(weather_info.text.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolates weather tweet\n",
    "for value in reversed(weather_info_list):                                    \n",
    "    if value[:3]=='Sol':                                                     \n",
    "        mars_weather = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sol 2171 (2018-09-14), high -12C/10F, low -65C/-84F, pressure at 8.79 hPa, daylight 05:43-17:59'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#did it work?\n",
    "mars_weather                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up by closing the automated browser\n",
    "browser.quit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>Equatorial Diameter:</td>\n",
      "      <td>6,792 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Polar Diameter:</td>\n",
      "      <td>6,752 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mass:</td>\n",
      "      <td>6.42 x 10^23 kg (10.7% Earth)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Moons:</td>\n",
      "      <td>2 (Phobos &amp; Deimos)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orbit Distance:</td>\n",
      "      <td>227,943,824 km (1.52 AU)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orbit Period:</td>\n",
      "      <td>687 days (1.9 years)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Surface Temperature:</td>\n",
      "      <td>-153 to 20 Â°C</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>First Record:</td>\n",
      "      <td>2nd millennium BC</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Recorded By:</td>\n",
      "      <td>Egyptian astronomers</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "#Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts \n",
    "#about the planet including Diameter, Mass, etc.\n",
    "#defining, extracting, and converting to a dataframe for eaiser use\n",
    "facts_url = 'https://space-facts.com/mars/' \n",
    "fact_list = pd.read_html(facts_url)  \n",
    "facts_df = fact_list[0]   \n",
    "\n",
    "# converts dataframe to html table\n",
    "facts_table = facts_df.to_html(header=False, index=False)                    \n",
    "print(facts_table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting high resolution images for each of Mars' hemispheres\n",
    "\n",
    "#define browser again.\n",
    "browser = Browser('chrome', headless=False)                                                                  \n",
    "\n",
    "# defines search URL\n",
    "usgs_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(usgs_url)\n",
    "usgs_html = browser.html                                                     \n",
    "usgs_soup = bs(usgs_html, 'html.parser')  \n",
    "hemisphere_image_urls = []                                             \n",
    "\n",
    "products = usgs_soup.find('div', class_='result-list')                       \n",
    "hemispheres = products.find_all('div', class_='item')                        \n",
    "\n",
    "for hemisphere in hemispheres:                                             \n",
    "    title = hemisphere.find('div', class_='description')\n",
    "    \n",
    "    title_text = title.a.text                                                \n",
    "    title_text = title_text.replace(' Enhanced', '')\n",
    "    browser.click_link_by_partial_text(title_text)                           \n",
    "    \n",
    "    usgs_html = browser.html                                                 \n",
    "    usgs_soup = bs(usgs_html, 'html.parser')                                 \n",
    "    \n",
    "    image = usgs_soup.find('div', class_='downloads').find('ul').find('li')  \n",
    "    img_url = image.a['href']\n",
    "    \n",
    "    hemisphere_image_urls.append({'title': title_text, 'img_url': img_url})    \n",
    "    \n",
    "    browser.click_link_by_partial_text('Back') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all my image dictionaries\n",
    "browser.quit()  \n",
    "hemisphere_image_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
